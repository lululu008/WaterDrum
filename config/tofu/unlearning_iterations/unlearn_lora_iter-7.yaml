pretrained_model_name_or_path: meta-llama/Llama-2-7b-chat-hf
model_family: llama2-7b

lora:
    r: 8
    lora_alpha: 32
    lora_dropout: 0.05
    bias: none
    task_type: CAUSAL_LM

train_batch_size: 16
eval_batch_size: 32
gradient_accumulation_steps: 1
learning_rate: 0.00001
weight_decay: 0
num_epochs: 7
generation_max_length: 200
max_seq_length: 512

finetune:
    learning_rate: 0.0001

tv:
    learning_rate: 0.0001

scrub:
    num_epochs: 1
    num_total_epochs: 7
    num_maximize_epochs: 1